{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Preparación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se entrenan **distintos modelos** sobre los datos limpios con el objetivo de **predecir el número de ventas para el mes de noviembre de 2015 por tienda y artículo**. Se prueban los modelos de **regresión con regularización ridge, regularización lasso y XGBoost**.\n",
    "\n",
    "Es importante desctacar que en la sección anterior de Procesamiento de Datos, se logró obtener la base de datos limpia que sirve para entrenar modelos utilizando R. Sin embargo, el data frame con las **10,913,850 observaciones y 44 columnas** que se obtuvieron como resultado y la manera en que R guarda y escribe CSVs, se generarón archivos muy pesados. En consecuencia, leer la base limpia en Python y Jupyter Notebook ocasionaba problemas de memoría, matando al Kernel.\n",
    "\n",
    "Tomando en cuenta lo anterior y sin olvidar la instrucción inicial de limpiar los datos en R, en este documento de modelado se replica el código de limpieza que se obtuvó y se describió en la sección de Procesamiento de Datos. Esta implementación generar el mismo dataframe, pero optimizado y con menor peso que funciona bien en Python para entrenar modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos las librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las librerías que se utilizan para el modelado son las siguientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos los datos crudos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'datos'\n",
    "transactions    = pd.read_csv(os.path.join(DATA_FOLDER, 'sales_train.csv'))\n",
    "items           = pd.read_csv(os.path.join(DATA_FOLDER, 'items.csv'))\n",
    "item_categories = pd.read_csv(os.path.join(DATA_FOLDER, 'item_categories.csv'), encoding = \"Windows-1251\")\n",
    "shops           = pd.read_csv(os.path.join(DATA_FOLDER, 'shops.csv'))\n",
    "test            = pd.read_csv(os.path.join(DATA_FOLDER, 'test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generamos los datos limpios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problemas:**\n",
    "\n",
    "* No todos los artículos se venden en las mismas tiendas, todos los meses\n",
    "* Matriz rala\n",
    "\n",
    "**Solución:**\n",
    "    \n",
    "* Completar con un grid\n",
    "* Imputar ceros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d285504054d028521de3e90def1bd88e3521695f"
   },
   "source": [
    "Un problema es que hay duplas (tienda, artículo) para las cuales no hay registro en algunos meses. Es decir, no en todos los meses del año se vendieron todos los artículos en todas las tiendas. Por lo tanto, no se tienen las series de tiempo completas. Para resolver este problema, vamos a completar dichas series de tal manera que el algoritmo sea capaz de aprender las irregularidades en las ventas de 1C.\n",
    "\n",
    "Generamos el grid con todas las combinaciones posibles de mes, tienda y artículo que dió como resultado un dataset de 10,913,850 de filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos un merge de las los itemas a las transacciones por item id\n",
    "transactions = pd.merge(transactions, items, on='item_id', how='left')\n",
    "transactions = transactions.drop('item_name', axis=1)\n",
    "transactions.head()\n",
    "\n",
    "# Construimos el grid \n",
    "from itertools import product\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "# Inicializas array\n",
    "grid = []\n",
    "# Para cada mes sacar tiendas e items únicos, calculas el plano cartesiano y lo guardas\n",
    "for block_num in transactions['date_block_num'].unique():\n",
    "    cur_shops = transactions.loc[transactions['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = transactions.loc[transactions['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])), dtype='int32'))\n",
    "# Apilas cada uno de los planos cartesianos para tener el grid    \n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos las características de ventas promedio por mes, tienda y producto con las cuáles se crean las características que necesitamos de manera rezagada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "2f0722af224de94d0fcf48e3569c67249aa13fc9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cálculamos medias por mes, tienda y producto.\n",
    "mean_transactions = transactions.groupby(['date_block_num', 'shop_id', 'item_id']).agg(\n",
    "    {'item_cnt_day':'sum','item_price':np.mean}).reset_index()\n",
    "# Pegamos estos promedios al grid\n",
    "mean_transactions = pd.merge(grid,mean_transactions,on=['date_block_num', 'shop_id', 'item_id'],how='left')\n",
    "\n",
    "# Se juntan estas estadísticas al grid\n",
    "mean_transactions = pd.merge(grid,mean_transactions,on=['date_block_num', 'shop_id', 'item_id'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unicamente tenemos missings en item_price_mean y item_cnt_month. Los missings en item_cnt_month corresponden a que no hubo ventas para ese artículo, en ese mes, en esa tienda; porlo tanto, sustituimos estos NA's con 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "4b1e0aa6302e259c9cd3b3ed7880ac168a48dc70"
   },
   "outputs": [],
   "source": [
    "# Imputamos con ceros\n",
    "mean_transactions = pd.merge(grid,mean_transactions,on=['date_block_num', 'shop_id', 'item_id'],how='left').fillna(0)\n",
    "# Se juntan los datos de categorías de artículo\n",
    "mean_transactions = pd.merge(mean_transactions, items, on='item_id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingeniería de características\n",
    "\n",
    "Se generan **9 nuevas variables** que serán utilizadas posteriormente para obtener los reszagos de interes.\n",
    "\n",
    "**Por artículo de interés en todas las tiendas**:\n",
    "\n",
    "* precio promedio\n",
    "* ventas totales\n",
    "* ventas promedio\n",
    "\n",
    "**Por tienda de interés**:\n",
    "\n",
    "* precio promedio del producto\n",
    "* ventas totales del producto\n",
    "* ventas promedio del producto\n",
    "\n",
    "**Por categoría del producto de interés**: \n",
    "\n",
    "* precio promedio\n",
    "* ventas totales\n",
    "* ventas promedio\n",
    "\n",
    "\n",
    "En total se crean 36 nuevas variables. De tal manera que nuestra **matriz de diseño queda de 10,849,431 filas y 44 columnas**. \n",
    "\n",
    "<img src=\"img/feature_construction.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los datos originales creamos las siguientes características:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "91c6985c5892fad337f669aeae7d7b37c6163edc"
   },
   "outputs": [],
   "source": [
    "# El proceso se repite por type_id\n",
    "for type_id in ['item_id', 'shop_id', 'item_category_id']:\n",
    "    # Se van a calcular usando una tupla el precio promedio, las ventas totales, las ventas diarias\n",
    "    for column_id, aggregator, aggtype in [('item_price',np.mean,'avg'),('item_cnt_day',np.sum,'sum'),('item_cnt_day',np.mean,'avg')]:\n",
    "        # Agregas el grid con las medias por la categoría de agregación type_id\n",
    "        mean_df = transactions.groupby([type_id,'date_block_num']).aggregate(aggregator).reset_index()[[column_id,type_id,'date_block_num']]\n",
    "        # A estas nuevas variables, les asignas su nombre por categoría\n",
    "        mean_df.columns = [type_id+'_'+aggtype+'_'+column_id,type_id,'date_block_num']\n",
    "        # Agregas características al grid\n",
    "        mean_transactions = pd.merge(mean_transactions, mean_df, on=['date_block_num',type_id], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las variables que se crearon se agregaron al dataset completo con un left join. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que en la verdadera tarea de predicción no se van a tener los datos actuales, sino los datos de los meses previos, creamos rezagos mensuales, bimestrales, trimestrales y semestrales de las variables de precio y cantidad creadas en el paso anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "aee75fffdadbf01e528f3e354d34ea61102e93b2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef304a794a8f4cd38df3e2bbc21dd7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Se crean las características rezagadas\n",
    "lag_variables  = list(mean_transactions.columns[7:])+['item_cnt_day']\n",
    "# Creamos 4 lags\n",
    "lags = [1, 2, 3, 6]\n",
    "from tqdm import tqdm_notebook\n",
    "# Loop por lag\n",
    "for lag in tqdm_notebook(lags):\n",
    "    # Creas una copia del grid populado\n",
    "    sales_new_df = mean_transactions.copy()\n",
    "    # Incrementas la indicadora del mes por el lag\n",
    "    sales_new_df.date_block_num += lag\n",
    "    # Extraes las variables categóricas de identificación más los lags\n",
    "    sales_new_df = sales_new_df[['date_block_num','shop_id','item_id']+lag_variables]\n",
    "    # Cambias los nombres para que indiquen a que lag pertenecen\n",
    "    sales_new_df.columns = ['date_block_num','shop_id','item_id']+ [lag_feat+'_lag_'+str(lag) for lag_feat in lag_variables]\n",
    "    # Haces un merge al grid populado\n",
    "    mean_transactions = pd.merge(mean_transactions, sales_new_df,on=['date_block_num','shop_id','item_id'] ,how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al generar un rezago de tamaño $n$ se obtienene $n$ valores faltantes. Imputamos los NA's de los rezagos (primeros $n$ valores para rezago de tamaño $n$ en cada variable) con el primer valor observado (correspondiente a la observación $n+1$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "9a2f9585ea59086c4ec3de28faa4ee504e8cda94"
   },
   "outputs": [],
   "source": [
    "#  Imputación de NAs después de rezagar\n",
    "mean_transactions = mean_transactions[mean_transactions['date_block_num'] > 12]\n",
    "# Rellenamos con ceros item_cnt\n",
    "# Rellenamos con medianas los precios\n",
    "for feat in mean_transactions.columns:\n",
    "    if 'item_cnt' in feat:\n",
    "        mean_transactions[feat]=mean_transactions[feat].fillna(0)\n",
    "    elif 'item_price' in feat:\n",
    "        mean_transactions[feat]=mean_transactions[feat].fillna(mean_transactions[feat].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se quitan todos los datos que no tendremos al momento de hacer la predicción, es decir todos los datos sin rezago. Solo se conserva \"item_cnt_day\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "47443d0b3d9686f94f0e036d6b2f7e84462818f9"
   },
   "outputs": [],
   "source": [
    "cols_to_drop = lag_variables[:-1] + ['item_price', 'item_name']\n",
    "training = mean_transactions.drop(cols_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las columnas del dataframe de entrenamiento limpio y con características nuevas se muestra a continuación.\n",
    "\n",
    "*Información del producto, tienda y mes de interés*\n",
    "\n",
    "1. **shop_id**: Id de la tienda\n",
    "2. **item_id**: Id del producto\n",
    "3. **date_block_num**: Id del mes, en este caso 33 para el mes de octubre de 2015\n",
    "4. **item_cnt_day**: Ventas promedio mensuales para el artículo\n",
    "5. **item_category_id**: Id de la categoría del producto\n",
    "\n",
    "*Rezago mensual: datos agrupados por producto, tienda y categoría*\n",
    "\n",
    "6. **item_id_avg_item_price_lag_1**: artículo de interés en todas las tiendas, precio promedio del mes anterior\n",
    "7. **item_id_sum_item_cnt_day_lag_1**: artículo de interés en todas las tiendas, ventas totales del mes anterior\n",
    "8. **item_id_avg_item_cnt_day_lag_1**: artículo de interés en todas las tiendas, ventas promedio del mes anterior\n",
    "9. **shop_id_avg_item_price_lag_1**: en la tienda de interés, precio promedio del producto en el mes anterior.\n",
    "10. **shop_id_sum_item_cnt_day_lag_1**: en la tienda de interés, ventas totales del producto en el mes anterior.\n",
    "11. **shop_id_avg_item_cnt_day_lag_1**: en la tienda de interés, ventas promedio del producto en el mes anterior.\n",
    "12. **item_category_id_avg_item_price_lag_1**: en la categoría del producto de interés, precio promedio en el mes anterior.\n",
    "13. **item_category_id_sum_item_cnt_day_lag_1**: en la categoría del producto de interés, ventas totales en mes anterior.\n",
    "14. **item_category_id_avg_item_cnt_day_lag_1**: en la categoría del producto de interés, ventas promedio del mes anterior.\n",
    "15. **item_cnt_day_lag_1**: artículo y tienda de interés, ventas en el mes anterior en la tienda de in \n",
    "\n",
    "*Rezago bimestral: datos agrupados por producto, tienda y categoría*\n",
    "\n",
    "16. **item_id_avg_item_price_lag_2**: ídem, dos meses atrás\n",
    "17. **item_id_sum_item_cnt_day_lag_2**: ídem, dos meses atrás\n",
    "18. **item_id_avg_item_cnt_day_lag_2**: ídem, dos meses atrás\n",
    "19. **shop_id_avg_item_price_lag_2**: ídem, dos meses atrás\n",
    "20. **shop_id_sum_item_cnt_day_lag_2**: ídem, dos meses atrás\n",
    "21. **shop_id_avg_item_cnt_day_lag_2**: ídem, dos meses atrás\n",
    "22. **item_category_id_avg_item_price_lag_2**: ídem, dos meses atrás\n",
    "23. **item_category_id_sum_item_cnt_day_lag_2**: ídem, dos meses atrás\n",
    "24. **item_category_id_avg_item_cnt_day_lag_2**: ídem, dos meses atrás\n",
    "25. **item_cnt_day_lag_2**:\n",
    "\n",
    "*Rezago cuatrimestral: datos agrupados por producto, tienda y categoría*\n",
    "\n",
    "26. **item_id_avg_item_price_lag_3**: ídem, tres meses atrás\n",
    "27. **item_id_sum_item_cnt_day_lag_3**: ídem, tres meses atrás\n",
    "28. **item_id_avg_item_cnt_day_lag_3**: ídem, tres meses atrás\n",
    "29. **shop_id_avg_item_price_lag_3**: ídem, tres meses atrás\n",
    "30. **shop_id_sum_item_cnt_day_lag_3**: ídem, tres meses atrás\n",
    "31. **shop_id_avg_item_cnt_day_lag_3**: ídem, tres meses atrás\n",
    "32. **item_category_id_avg_item_price_lag_3**: ídem, tres meses atrás\n",
    "33. **item_category_id_sum_item_cnt_day_lag_3**: ídem, tres meses atrás\n",
    "34. **item_category_id_avg_item_cnt_day_lag_3**: ídem, tres meses atrás\n",
    "35. item_cnt_day_lag_3**: ídem, tres meses atrás\n",
    "\n",
    "*Rezago semestral: datos agrupados por producto, tienda y categoría*\n",
    "\n",
    "36. **item_id_avg_item_price_lag_6**: ídem, seis meses atrás\n",
    "37. **item_id_sum_item_cnt_day_lag_6**: ídem, seis meses atrás\n",
    "38. **item_id_avg_item_cnt_day_lag_6**: ídem, seis meses atrás\n",
    "39. **shop_id_avg_item_price_lag_6**: ídem, seis meses atrás\n",
    "40. **shop_id_sum_item_cnt_day_lag_6**: ídem, seis meses atrás\n",
    "41. **shop_id_avg_item_cnt_day_lag_6**: ídem, seis meses atrás\n",
    "42. **item_category_id_avg_item_price_lag_6**: ídem, seis meses atrás\n",
    "43. **item_category_id_sum_item_cnt_day_lag_6**: ídem, seis meses atrás\n",
    "44. **item_category_id_avg_item_cnt_day_lag_6**: ídem, seis meses atrás\n",
    "45. **item_cnt_day_lag_6**: ídem, tres meses atrás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos una copia el directorio de modelos\n",
    "training.to_csv(\"Data_Modelos/Entrenamiento_Modelos2.csv\", index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Modelado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de modelos\n",
    "\n",
    "A partir de esta parte del documento se procede a entrenar los modelos. Primero se procesan los datos en el formato que requieren las librerías de los modelos, se estandarizan las variables explicativas y se realiza una validación cruzada para encontrar el mejor modelo.\n",
    "\n",
    "Se ajustará primero una regresión ridge, luego una lasso y finalmente un xgboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos la base de datos de entrenamiento limpia y con rezagos.\n",
    "#training = pd.read_csv(\"Data_Modelos/Entrenamiento_Modelos2.csv\", header=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos el conjunto de entrenamiento en la matriz de variables explicativas y el vector de variables respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos el conjunto de entrenamiento en y y X \n",
    "X = training.iloc[:, training.columns != 'item_cnt_day'].values \n",
    "y = training.iloc[:, training.columns == 'item_cnt_day'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estandarizamos datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aplica un procedimiento de estandarización de las variables explicativas. Esto con el objeto de ayudar a encontrar un valor óptimo de los parámetros de manera eficiente y también porque así lo requieren los modelos de regularización Ridge y Lasso. \n",
    "\n",
    "En este proyecto se utiliza el método restar la media y dividir por la norma l2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalamiento en la matriz de diseño.\n",
    "#scaler = MinMaxScaler()\n",
    "#X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción de conjuntos de valiadción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evitar el sobre ajuste de los modelos y poder comparar su efectividad para predecir observaciones no antes vistas, el proceso de validación cruzada que se lleva acabo para un problema de series de tiempo.\n",
    "\n",
    "Se parte el conjunto de entrenamiento en **5 splits de diferente longitud**, como se muestra en la imagen. Cada split se va incrementando en el tiempo para reflejar el efecto temporal de las series. **Cada split tiene su conjunto de entrenamiento y validación**. \n",
    "\n",
    "Al final el error de prueba se calcula promediando el error de prueba de cada split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"Data_Modelos/crossvalidation.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos conjuntos de validación cruzada\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "my_cv = TimeSeriesSplit(n_splits=5).split(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Modelo de Regresión Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ajusta un modelo de regresión lineal con regularización ridge, para penalizar los modelos con parámetros muy grandes. \n",
    "\n",
    "* Se prueba distintos parámetros de **alpha 0.1, 1, 10, 20 y 30** donde a más grande el alpha más se castiga aquellos parámetros con valores altos. Entre más cercano el parámetro de alpha a cero, más parecido el resultado es a la regresión lineal.\n",
    "\n",
    "* Se utilizó la función de pérdida \"negative mean squared error\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=<generator object TimeSeriesSplit.split at 0x000002430C4C7780>,\n",
       "       error_score='raise',\n",
       "       estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=True, random_state=None, solver='auto', tol=0.001),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': [0.1, 1, 10, 20, 30]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelos con Validación Cruzada con Grid Search\n",
    "## Ridge\n",
    "ridge = Ridge(normalize=True)\n",
    "## 5 splits\n",
    "my_cv = TimeSeriesSplit(n_splits=5).split(X)\n",
    "## regularización\n",
    "param_grid = {'alpha': [0.1, 1, 10, 20, 30]}\n",
    "## grid search\n",
    "\n",
    "ridge_cv = GridSearchCV(estimator = ridge,\n",
    "                        param_grid = param_grid,\n",
    "                        scoring = 'neg_mean_squared_error',\n",
    "                        cv = my_cv\n",
    "                        )\n",
    "ridge_cv.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados de la validación son los siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.563997</td>\n",
       "      <td>2.012142</td>\n",
       "      <td>0.120198</td>\n",
       "      <td>0.017758</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "      <td>-2.347157</td>\n",
       "      <td>-6.743744</td>\n",
       "      <td>-9.106981</td>\n",
       "      <td>-9.368888</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.531836</td>\n",
       "      <td>4.135702</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.744499</td>\n",
       "      <td>-2.993961</td>\n",
       "      <td>-4.165695</td>\n",
       "      <td>-5.375480</td>\n",
       "      <td>-6.160724</td>\n",
       "      <td>-4.488072</td>\n",
       "      <td>1.137933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.621394</td>\n",
       "      <td>2.088545</td>\n",
       "      <td>0.113001</td>\n",
       "      <td>0.013040</td>\n",
       "      <td>1</td>\n",
       "      <td>{'alpha': 1}</td>\n",
       "      <td>-2.541313</td>\n",
       "      <td>-7.648511</td>\n",
       "      <td>-10.015086</td>\n",
       "      <td>-8.904451</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.951469</td>\n",
       "      <td>4.213421</td>\n",
       "      <td>2</td>\n",
       "      <td>-4.212809</td>\n",
       "      <td>-3.366871</td>\n",
       "      <td>-4.618107</td>\n",
       "      <td>-5.920064</td>\n",
       "      <td>-6.641781</td>\n",
       "      <td>-4.951926</td>\n",
       "      <td>1.180087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.886791</td>\n",
       "      <td>2.081479</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>0.011333</td>\n",
       "      <td>10</td>\n",
       "      <td>{'alpha': 10}</td>\n",
       "      <td>-4.513691</td>\n",
       "      <td>-10.705913</td>\n",
       "      <td>-14.344333</td>\n",
       "      <td>-9.326813</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.275818</td>\n",
       "      <td>4.424047</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.861300</td>\n",
       "      <td>-5.693343</td>\n",
       "      <td>-7.184180</td>\n",
       "      <td>-8.916298</td>\n",
       "      <td>-9.240763</td>\n",
       "      <td>-7.579177</td>\n",
       "      <td>1.324877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.790578</td>\n",
       "      <td>2.024215</td>\n",
       "      <td>0.112600</td>\n",
       "      <td>0.014854</td>\n",
       "      <td>20</td>\n",
       "      <td>{'alpha': 20}</td>\n",
       "      <td>-5.357072</td>\n",
       "      <td>-11.748445</td>\n",
       "      <td>-15.907999</td>\n",
       "      <td>-9.848393</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.192559</td>\n",
       "      <td>4.497311</td>\n",
       "      <td>4</td>\n",
       "      <td>-7.976637</td>\n",
       "      <td>-6.678059</td>\n",
       "      <td>-8.240841</td>\n",
       "      <td>-10.120078</td>\n",
       "      <td>-10.251115</td>\n",
       "      <td>-8.653346</td>\n",
       "      <td>1.358972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.762989</td>\n",
       "      <td>2.026495</td>\n",
       "      <td>0.111600</td>\n",
       "      <td>0.011842</td>\n",
       "      <td>30</td>\n",
       "      <td>{'alpha': 30}</td>\n",
       "      <td>-5.772469</td>\n",
       "      <td>-12.239366</td>\n",
       "      <td>-16.651201</td>\n",
       "      <td>-10.132785</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.636634</td>\n",
       "      <td>4.532799</td>\n",
       "      <td>5</td>\n",
       "      <td>-8.526545</td>\n",
       "      <td>-7.160795</td>\n",
       "      <td>-8.756514</td>\n",
       "      <td>-10.702955</td>\n",
       "      <td>-10.734824</td>\n",
       "      <td>-9.176327</td>\n",
       "      <td>1.372613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
       "0       8.563997      2.012142         0.120198        0.017758         0.1   \n",
       "1       8.621394      2.088545         0.113001        0.013040           1   \n",
       "2       8.886791      2.081479         0.112000        0.011333          10   \n",
       "3       8.790578      2.024215         0.112600        0.014854          20   \n",
       "4       8.762989      2.026495         0.111600        0.011842          30   \n",
       "\n",
       "           params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0  {'alpha': 0.1}          -2.347157          -6.743744          -9.106981   \n",
       "1    {'alpha': 1}          -2.541313          -7.648511         -10.015086   \n",
       "2   {'alpha': 10}          -4.513691         -10.705913         -14.344333   \n",
       "3   {'alpha': 20}          -5.357072         -11.748445         -15.907999   \n",
       "4   {'alpha': 30}          -5.772469         -12.239366         -16.651201   \n",
       "\n",
       "   split3_test_score       ...         mean_test_score  std_test_score  \\\n",
       "0          -9.368888       ...               -8.531836        4.135702   \n",
       "1          -8.904451       ...               -8.951469        4.213421   \n",
       "2          -9.326813       ...              -11.275818        4.424047   \n",
       "3          -9.848393       ...              -12.192559        4.497311   \n",
       "4         -10.132785       ...              -12.636634        4.532799   \n",
       "\n",
       "   rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0                1           -3.744499           -2.993961   \n",
       "1                2           -4.212809           -3.366871   \n",
       "2                3           -6.861300           -5.693343   \n",
       "3                4           -7.976637           -6.678059   \n",
       "4                5           -8.526545           -7.160795   \n",
       "\n",
       "   split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0           -4.165695           -5.375480           -6.160724   \n",
       "1           -4.618107           -5.920064           -6.641781   \n",
       "2           -7.184180           -8.916298           -9.240763   \n",
       "3           -8.240841          -10.120078          -10.251115   \n",
       "4           -8.756514          -10.702955          -10.734824   \n",
       "\n",
       "   mean_train_score  std_train_score  \n",
       "0         -4.488072         1.137933  \n",
       "1         -4.951926         1.180087  \n",
       "2         -7.579177         1.324877  \n",
       "3         -8.653346         1.358972  \n",
       "4         -9.176327         1.372613  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(ridge_cv.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mejor modelo en términos de la suma de error medio al cuadrado es el que usa una alpha de regularización de 0.1. La suma del error medio al cuadrado fue 8.5318. La raíz del error medio al cuadrado (RMSE) es: 2.9209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9209307372490954"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(-1 * ridge_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Modelo de Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ajusta una regresión con regularización lasso para **penalizar por parámetros muy grandes y en su caso hacer 0 algunas caraterísticas**. Se prueban distintas **alphas 0.1, 1, 10, 20 y 30** donde a más grande el alpha más se castiga aquellos parámetros con valores altos. Entre más cercano el parámetro de alpha a cero, más parecido el resultado es a la regresión lineal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=<generator object TimeSeriesSplit.split at 0x000002430C4D9468>,\n",
       "       error_score='raise',\n",
       "       estimator=Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=True, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': [0.1, 1, 10, 20, 30]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lasso\n",
    "lasso = Lasso(normalize=True)\n",
    "my_cv = TimeSeriesSplit(n_splits=5).split(X)\n",
    "param_grid = {'alpha': [0.1, 1, 10, 20, 30]}\n",
    "lasso_cv = GridSearchCV(estimator = lasso,\n",
    "                        param_grid = param_grid,\n",
    "                        scoring = 'neg_mean_squared_error',\n",
    "                        cv = my_cv)\n",
    "lasso_cv.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados de los 5 modelos se encuentran a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.195976</td>\n",
       "      <td>3.496058</td>\n",
       "      <td>0.108002</td>\n",
       "      <td>0.010099</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "      <td>-6.978841</td>\n",
       "      <td>-13.603655</td>\n",
       "      <td>-18.732158</td>\n",
       "      <td>-11.022038</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.902683</td>\n",
       "      <td>4.636086</td>\n",
       "      <td>1</td>\n",
       "      <td>-10.124005</td>\n",
       "      <td>-8.551171</td>\n",
       "      <td>-10.235259</td>\n",
       "      <td>-12.359044</td>\n",
       "      <td>-12.091605</td>\n",
       "      <td>-10.672217</td>\n",
       "      <td>1.403592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.549454</td>\n",
       "      <td>3.440850</td>\n",
       "      <td>0.109800</td>\n",
       "      <td>0.004261</td>\n",
       "      <td>1</td>\n",
       "      <td>{'alpha': 1}</td>\n",
       "      <td>-6.978841</td>\n",
       "      <td>-13.603655</td>\n",
       "      <td>-18.732158</td>\n",
       "      <td>-11.022038</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.902683</td>\n",
       "      <td>4.636086</td>\n",
       "      <td>1</td>\n",
       "      <td>-10.124005</td>\n",
       "      <td>-8.551171</td>\n",
       "      <td>-10.235259</td>\n",
       "      <td>-12.359044</td>\n",
       "      <td>-12.091605</td>\n",
       "      <td>-10.672217</td>\n",
       "      <td>1.403592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.372269</td>\n",
       "      <td>3.580016</td>\n",
       "      <td>0.108001</td>\n",
       "      <td>0.004648</td>\n",
       "      <td>10</td>\n",
       "      <td>{'alpha': 10}</td>\n",
       "      <td>-6.978841</td>\n",
       "      <td>-13.603655</td>\n",
       "      <td>-18.732158</td>\n",
       "      <td>-11.022038</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.902683</td>\n",
       "      <td>4.636086</td>\n",
       "      <td>1</td>\n",
       "      <td>-10.124005</td>\n",
       "      <td>-8.551171</td>\n",
       "      <td>-10.235259</td>\n",
       "      <td>-12.359044</td>\n",
       "      <td>-12.091605</td>\n",
       "      <td>-10.672217</td>\n",
       "      <td>1.403592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.361865</td>\n",
       "      <td>3.314325</td>\n",
       "      <td>0.104601</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>20</td>\n",
       "      <td>{'alpha': 20}</td>\n",
       "      <td>-6.978841</td>\n",
       "      <td>-13.603655</td>\n",
       "      <td>-18.732158</td>\n",
       "      <td>-11.022038</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.902683</td>\n",
       "      <td>4.636086</td>\n",
       "      <td>1</td>\n",
       "      <td>-10.124005</td>\n",
       "      <td>-8.551171</td>\n",
       "      <td>-10.235259</td>\n",
       "      <td>-12.359044</td>\n",
       "      <td>-12.091605</td>\n",
       "      <td>-10.672217</td>\n",
       "      <td>1.403592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.233864</td>\n",
       "      <td>3.187917</td>\n",
       "      <td>0.106201</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>30</td>\n",
       "      <td>{'alpha': 30}</td>\n",
       "      <td>-6.978841</td>\n",
       "      <td>-13.603655</td>\n",
       "      <td>-18.732158</td>\n",
       "      <td>-11.022038</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.902683</td>\n",
       "      <td>4.636086</td>\n",
       "      <td>1</td>\n",
       "      <td>-10.124005</td>\n",
       "      <td>-8.551171</td>\n",
       "      <td>-10.235259</td>\n",
       "      <td>-12.359044</td>\n",
       "      <td>-12.091605</td>\n",
       "      <td>-10.672217</td>\n",
       "      <td>1.403592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
       "0      11.195976      3.496058         0.108002        0.010099         0.1   \n",
       "1      11.549454      3.440850         0.109800        0.004261           1   \n",
       "2      11.372269      3.580016         0.108001        0.004648          10   \n",
       "3      11.361865      3.314325         0.104601        0.002577          20   \n",
       "4      11.233864      3.187917         0.106201        0.003868          30   \n",
       "\n",
       "           params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0  {'alpha': 0.1}          -6.978841         -13.603655         -18.732158   \n",
       "1    {'alpha': 1}          -6.978841         -13.603655         -18.732158   \n",
       "2   {'alpha': 10}          -6.978841         -13.603655         -18.732158   \n",
       "3   {'alpha': 20}          -6.978841         -13.603655         -18.732158   \n",
       "4   {'alpha': 30}          -6.978841         -13.603655         -18.732158   \n",
       "\n",
       "   split3_test_score       ...         mean_test_score  std_test_score  \\\n",
       "0         -11.022038       ...              -13.902683        4.636086   \n",
       "1         -11.022038       ...              -13.902683        4.636086   \n",
       "2         -11.022038       ...              -13.902683        4.636086   \n",
       "3         -11.022038       ...              -13.902683        4.636086   \n",
       "4         -11.022038       ...              -13.902683        4.636086   \n",
       "\n",
       "   rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0                1          -10.124005           -8.551171   \n",
       "1                1          -10.124005           -8.551171   \n",
       "2                1          -10.124005           -8.551171   \n",
       "3                1          -10.124005           -8.551171   \n",
       "4                1          -10.124005           -8.551171   \n",
       "\n",
       "   split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0          -10.235259          -12.359044          -12.091605   \n",
       "1          -10.235259          -12.359044          -12.091605   \n",
       "2          -10.235259          -12.359044          -12.091605   \n",
       "3          -10.235259          -12.359044          -12.091605   \n",
       "4          -10.235259          -12.359044          -12.091605   \n",
       "\n",
       "   mean_train_score  std_train_score  \n",
       "0        -10.672217         1.403592  \n",
       "1        -10.672217         1.403592  \n",
       "2        -10.672217         1.403592  \n",
       "3        -10.672217         1.403592  \n",
       "4        -10.672217         1.403592  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lasso_cv.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mejor modelo resultó ser el que utiliza una alpha de regularización 0.1 que obtuvó el menor MSE de 13.9026. De esta manera, el RMSE promedio de la validación cruzada del mejor modelo regresión lasso fue de:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7286301849512595"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(-1 * lasso_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2d229849109f3a5a6bd071bf32510ed88fa1dcc3"
   },
   "source": [
    "###  Extreme Gradient Boosting Machine (XGBoost) model\n",
    "Se ajusta un XGBOOST para un problema de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parámetros:\n",
    "\n",
    "* **Max depth**: desde árboles cortos y profundos\n",
    "* **n_esimators**: número de árboles a crear o boosting rounds\n",
    "* **learning rate**: velocidad de ajuste utilizando bayes learners adicionales\n",
    "* **subsample**: fracción del training set que puede ser utilizada para una boosting round. Valores chicos underfitting, valores grandes over fitting (entre 0  y 1)\n",
    "* **col samples**: no se uso porque daba malos resultados\n",
    "* **min child weight**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "params_grid = {\n",
    "    'max_depth' : [3,10],\n",
    "    'n_estimators' : [5, 10],  #, 10, 25, 50\n",
    "    'learning_rate' : [0.3], # , 0.03, 0.003, 0.0003, 0.00003\n",
    "    'subsample':[1],\n",
    "    'min_child_weight':[0.5]\n",
    "}\n",
    "\n",
    "params_fixed = {'objective':'reg:linear'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación\n",
    "my_cv = TimeSeriesSplit(n_splits=5).split(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración\n",
    "# Pérdiad la negative mean squared root, semilla\n",
    "bst_grid = GridSearchCV(\n",
    "    estimator = XGBRegressor(**params_fixed, seed = 25021987),\n",
    "                                param_grid = params_grid,\n",
    "                                cv = my_cv,\n",
    "                                n_jobs = -1,\n",
    "                                scoring = 'neg_mean_squared_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base_score',\n",
       " 'booster',\n",
       " 'colsample_bylevel',\n",
       " 'colsample_bytree',\n",
       " 'gamma',\n",
       " 'learning_rate',\n",
       " 'max_delta_step',\n",
       " 'max_depth',\n",
       " 'min_child_weight',\n",
       " 'missing',\n",
       " 'n_estimators',\n",
       " 'n_jobs',\n",
       " 'nthread',\n",
       " 'objective',\n",
       " 'random_state',\n",
       " 'reg_alpha',\n",
       " 'reg_lambda',\n",
       " 'scale_pos_weight',\n",
       " 'seed',\n",
       " 'silent',\n",
       " 'subsample']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(XGBRegressor().get_params().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:54:09] Tree method is automatically selected to be 'approx' for faster speed. to use old behavior(exact greedy algorithm on single machine), set tree_method to 'exact'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=<generator object TimeSeriesSplit.split at 0x000001F1AC0295C8>,\n",
       "       error_score='raise',\n",
       "       estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=25021987,\n",
       "       silent=True, subsample=1),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_depth': [3, 10], 'n_estimators': [5, 10], 'learning_rate': [0.3], 'subsample': [1], 'min_child_weight': [0.5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst_grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_child_weight</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>params</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60.945517</td>\n",
       "      <td>29.183403</td>\n",
       "      <td>2.738785</td>\n",
       "      <td>0.621296</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.3, 'max_depth': 3, 'min_ch...</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.636734</td>\n",
       "      <td>3.977369</td>\n",
       "      <td>2</td>\n",
       "      <td>-3.819979</td>\n",
       "      <td>-2.951654</td>\n",
       "      <td>-4.137040</td>\n",
       "      <td>-5.013986</td>\n",
       "      <td>-5.657646</td>\n",
       "      <td>-4.316061</td>\n",
       "      <td>0.940925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.373174</td>\n",
       "      <td>52.178992</td>\n",
       "      <td>2.078028</td>\n",
       "      <td>1.093748</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.3, 'max_depth': 3, 'min_ch...</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.218937</td>\n",
       "      <td>3.962476</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.260720</td>\n",
       "      <td>-2.499618</td>\n",
       "      <td>-3.660976</td>\n",
       "      <td>-4.346696</td>\n",
       "      <td>-4.932570</td>\n",
       "      <td>-3.740116</td>\n",
       "      <td>0.844370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163.753444</td>\n",
       "      <td>75.762594</td>\n",
       "      <td>0.955608</td>\n",
       "      <td>0.120729</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.3, 'max_depth': 10, 'min_c...</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.011429</td>\n",
       "      <td>4.069385</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.552818</td>\n",
       "      <td>-2.053900</td>\n",
       "      <td>-3.004359</td>\n",
       "      <td>-3.213227</td>\n",
       "      <td>-3.844762</td>\n",
       "      <td>-2.933813</td>\n",
       "      <td>0.605356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>265.178625</td>\n",
       "      <td>121.595908</td>\n",
       "      <td>1.131193</td>\n",
       "      <td>0.084365</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.3, 'max_depth': 10, 'min_c...</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.071174</td>\n",
       "      <td>3.595683</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.689430</td>\n",
       "      <td>-1.584405</td>\n",
       "      <td>-2.433892</td>\n",
       "      <td>-2.391216</td>\n",
       "      <td>-3.176760</td>\n",
       "      <td>-2.255141</td>\n",
       "      <td>0.577887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      60.945517     29.183403         2.738785        0.621296   \n",
       "1     103.373174     52.178992         2.078028        1.093748   \n",
       "2     163.753444     75.762594         0.955608        0.120729   \n",
       "3     265.178625    121.595908         1.131193        0.084365   \n",
       "\n",
       "  param_learning_rate param_max_depth param_min_child_weight  \\\n",
       "0                 0.3               3                    0.5   \n",
       "1                 0.3               3                    0.5   \n",
       "2                 0.3              10                    0.5   \n",
       "3                 0.3              10                    0.5   \n",
       "\n",
       "  param_n_estimators param_subsample  \\\n",
       "0                  5               1   \n",
       "1                 10               1   \n",
       "2                  5               1   \n",
       "3                 10               1   \n",
       "\n",
       "                                              params       ...         \\\n",
       "0  {'learning_rate': 0.3, 'max_depth': 3, 'min_ch...       ...          \n",
       "1  {'learning_rate': 0.3, 'max_depth': 3, 'min_ch...       ...          \n",
       "2  {'learning_rate': 0.3, 'max_depth': 10, 'min_c...       ...          \n",
       "3  {'learning_rate': 0.3, 'max_depth': 10, 'min_c...       ...          \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0        -8.636734        3.977369                2           -3.819979   \n",
       "1        -8.218937        3.962476                1           -3.260720   \n",
       "2        -9.011429        4.069385                3           -2.552818   \n",
       "3        -9.071174        3.595683                4           -1.689430   \n",
       "\n",
       "   split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0           -2.951654           -4.137040           -5.013986   \n",
       "1           -2.499618           -3.660976           -4.346696   \n",
       "2           -2.053900           -3.004359           -3.213227   \n",
       "3           -1.584405           -2.433892           -2.391216   \n",
       "\n",
       "   split4_train_score  mean_train_score  std_train_score  \n",
       "0           -5.657646         -4.316061         0.940925  \n",
       "1           -4.932570         -3.740116         0.844370  \n",
       "2           -3.844762         -2.933813         0.605356  \n",
       "3           -3.176760         -2.255141         0.577887  \n",
       "\n",
       "[4 rows x 25 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bst_grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: -8.63673, std: 3.97737, params: {'learning_rate': 0.3, 'max_depth': 3, 'min_child_weight': 0.5, 'n_estimators': 5, 'subsample': 1},\n",
       " mean: -8.21894, std: 3.96248, params: {'learning_rate': 0.3, 'max_depth': 3, 'min_child_weight': 0.5, 'n_estimators': 10, 'subsample': 1},\n",
       " mean: -9.01143, std: 4.06939, params: {'learning_rate': 0.3, 'max_depth': 10, 'min_child_weight': 0.5, 'n_estimators': 5, 'subsample': 1},\n",
       " mean: -9.07117, std: 3.59568, params: {'learning_rate': 0.3, 'max_depth': 10, 'min_child_weight': 0.5, 'n_estimators': 10, 'subsample': 1}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst_grid.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados del mejor modelo xgboost\n",
    "\n",
    "De los cuatro modelos para xgboost ajustados, a continuación se presentan los resultados del mejor, en términos de su ajuste al RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio CV MSE: -8.21893725103605\n",
      "Mejores parámetros\n",
      "\tlearning_rate:: 0.3\n",
      "\tmax_depth:: 3\n",
      "\tmin_child_weight:: 0.5\n",
      "\tn_estimators:: 10\n",
      "\tsubsample:: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Promedio CV MSE: {}\".format(bst_grid.best_score_))\n",
    "print(\"Mejores parámetros\")\n",
    "for key, value in bst_grid.best_params_.items():\n",
    "    print(\"\\t{}:: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "El mejor modelo obtuvó un RMSE de:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8653097563788807"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(8.21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados del modelado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A manera de resumen, se estimaron **14 modelos**. Se intentaron otras combinaciones de xgboost que por motivos de memoria no pudieron adjuntarse a la tabla anterior pero ninguno superó estos resultados. \n",
    "\n",
    "El mejor Ridge:\n",
    "* 2.92\n",
    "\n",
    "El mejor Lasso:\n",
    "* 3.72\n",
    "\n",
    "El mejor xgboost:\n",
    "* 2.86\n",
    "\n",
    "En la siguiente sección se procederá a ajustar este modelo de xgboost sobre todo el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "525c6fe678bacf9aee25cdc8284d1f53cc1bc492"
   },
   "source": [
    "## Construcción del conjunto de prueba\n",
    "\n",
    "A continuación se construye la base de datos de prueba que servirá para predecir. Las únicas variables proporcionadas por la competencia fueron:\n",
    "\n",
    "* shop\n",
    "* item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "_uuid": "4268373670bdf2578e0bab2fa981f6880e199a9b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  shop_id  item_id\n",
       "0   0        5     5037\n",
       "1   1        5     5320\n",
       "2   2        5     5233\n",
       "3   3        5     5232\n",
       "4   4        5     5268"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leer conjunto de prueba\n",
    "test = pd.read_csv(os.path.join(DATA_FOLDER, 'test.csv'))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ninguna de estas es la variable dependiente **item_cnt_day**.\n",
    "\n",
    "Para correr el modelo se necesitan 44 variables de entrada en el orden que se presenta a continuación:\n",
    "\n",
    "*Información del producto, tienda y mes de interés*\n",
    "\n",
    "1. **shop_id**: Id de la tienda\n",
    "2. **item_id**: Id del producto\n",
    "3. **date_block_num**: Id del mes, en este caso 33 para el mes de octubre de 2015\n",
    "4. **item_cnt_day**: Ventas promedio mensuales para el artículo\n",
    "5. **item_category_id**: Id de la categoría del producto\n",
    "\n",
    "*Rezago mensual: datos agrupados por producto, tienda y categoría*\n",
    "\n",
    "6. **item_id_avg_item_price_lag_1**: artículo de interés en todas las tiendas, precio promedio del mes anterior\n",
    "7. **item_id_sum_item_cnt_day_lag_1**: artículo de interés en todas las tiendas, ventas totales del mes anterior\n",
    "8. **item_id_avg_item_cnt_day_lag_1**: artículo de interés en todas las tiendas, ventas promedio del mes anterior\n",
    "9. **shop_id_avg_item_price_lag_1**: en la tienda de interés, precio promedio del producto en el mes anterior.\n",
    "10. **shop_id_sum_item_cnt_day_lag_1**: en la tienda de interés, ventas totales del producto en el mes anterior.\n",
    "11. **shop_id_avg_item_cnt_day_lag_1**: en la tienda de interés, ventas promedio del producto en el mes anterior.\n",
    "12. **item_category_id_avg_item_price_lag_1**: en la categoría del producto de interés, precio promedio en el mes anterior.\n",
    "13. **item_category_id_sum_item_cnt_day_lag_1**: en la categoría del producto de interés, ventas totales en mes anterior.\n",
    "14. **item_category_id_avg_item_cnt_day_lag_1**: en la categoría del producto de interés, ventas promedio del mes anterior.\n",
    "15. **item_cnt_day_lag_1**: artículo y tienda de interés, ventas en el mes anterior en la tienda de in \n",
    "\n",
    "*Rezago bimestral: datos agrupados por producto, tienda y categoría*\n",
    "\n",
    "16. **item_id_avg_item_price_lag_2**: ídem, dos meses atrás\n",
    "17. **item_id_sum_item_cnt_day_lag_2**: ídem, dos meses atrás\n",
    "18. **item_id_avg_item_cnt_day_lag_2**: ídem, dos meses atrás\n",
    "19. **shop_id_avg_item_price_lag_2**: ídem, dos meses atrás\n",
    "20. **shop_id_sum_item_cnt_day_lag_2**: ídem, dos meses atrás\n",
    "21. **shop_id_avg_item_cnt_day_lag_2**: ídem, dos meses atrás\n",
    "22. **item_category_id_avg_item_price_lag_2**: ídem, dos meses atrás\n",
    "23. **item_category_id_sum_item_cnt_day_lag_2**: ídem, dos meses atrás\n",
    "24. **item_category_id_avg_item_cnt_day_lag_2**: ídem, dos meses atrás\n",
    "25. **item_cnt_day_lag_2**:\n",
    "\n",
    "*Rezago cuatrimestral: datos agrupados por producto, tienda y categoría*\n",
    "\n",
    "26. **item_id_avg_item_price_lag_3**: ídem, tres meses atrás\n",
    "27. **item_id_sum_item_cnt_day_lag_3**: ídem, tres meses atrás\n",
    "28. **item_id_avg_item_cnt_day_lag_3**: ídem, tres meses atrás\n",
    "29. **shop_id_avg_item_price_lag_3**: ídem, tres meses atrás\n",
    "30. **shop_id_sum_item_cnt_day_lag_3**: ídem, tres meses atrás\n",
    "31. **shop_id_avg_item_cnt_day_lag_3**: ídem, tres meses atrás\n",
    "32. **item_category_id_avg_item_price_lag_3**: ídem, tres meses atrás\n",
    "33. **item_category_id_sum_item_cnt_day_lag_3**: ídem, tres meses atrás\n",
    "34. **item_category_id_avg_item_cnt_day_lag_3**: ídem, tres meses atrás\n",
    "35. item_cnt_day_lag_3**: ídem, tres meses atrás\n",
    "\n",
    "*Rezago semestral: datos agrupados por producto, tienda y categoría*\n",
    "\n",
    "36. **item_id_avg_item_price_lag_6**: ídem, seis meses atrás\n",
    "37. **item_id_sum_item_cnt_day_lag_6**: ídem, seis meses atrás\n",
    "38. **item_id_avg_item_cnt_day_lag_6**: ídem, seis meses atrás\n",
    "39. **shop_id_avg_item_price_lag_6**: ídem, seis meses atrás\n",
    "40. **shop_id_sum_item_cnt_day_lag_6**: ídem, seis meses atrás\n",
    "41. **shop_id_avg_item_cnt_day_lag_6**: ídem, seis meses atrás\n",
    "42. **item_category_id_avg_item_price_lag_6**: ídem, seis meses atrás\n",
    "43. **item_category_id_sum_item_cnt_day_lag_6**: ídem, seis meses atrás\n",
    "44. **item_category_id_avg_item_cnt_day_lag_6**: ídem, seis meses atrás\n",
    "45. **item_cnt_day_lag_6**: ídem, tres meses atrás\n",
    "\n",
    "Es necesario preprocesar el conjunto de prueba de manera similar al de entrenamiento. Por lo que es necesario agregar y utilizar las siguientes variables:\n",
    "\n",
    "*  date_block_num = 34 (noviembre de 2015)\n",
    "*  category_id\n",
    "*  lagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "_uuid": "2d2c3286613ab7c8ef00eea3e56d2b45c74cf121"
   },
   "outputs": [],
   "source": [
    "# Mes correspondiente a noviembre de 2015.\n",
    "test['date_block_num'] = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "_uuid": "6276776e003fd922d2c2a3b7a5976dc35a6f8f69"
   },
   "outputs": [],
   "source": [
    "# Pegamos catálogos\n",
    "test = pd.merge(test, items, on='item_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "_uuid": "7f3276ce857e6772dea4a70c55c699f03bc2fb21"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae7f7942124429fae4a435ae48227e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "# Pintar una barra de progreso en cada lag\n",
    "for lag in tqdm_notebook(lags):\n",
    "    # Crear rezagos con la indicadora del mes\n",
    "    sales_new_df = mean_transactions.copy()\n",
    "    sales_new_df.date_block_num += lag\n",
    "    # Seleccionar columnas\n",
    "    sales_new_df = sales_new_df[['date_block_num','shop_id','item_id']+lag_variables]\n",
    "    # Editar nombres de columnas\n",
    "    sales_new_df.columns = ['date_block_num','shop_id','item_id']+ [lag_feat+'_lag_'+str(lag) for lag_feat in lag_variables]\n",
    "    # Merge entre test y sales new\n",
    "    test = pd.merge(test, sales_new_df,on=['date_block_num','shop_id','item_id'] ,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "_uuid": "a9f55a37566f3c731a1200777e5f9cd9bf2d3ee5"
   },
   "outputs": [],
   "source": [
    "# Verificamos que las columnas del conjunto de entrenamiento y prueba esten en el mismo orden\n",
    "_test = set(test.drop(['ID', 'item_name'], axis=1).columns)\n",
    "_training = set(training.drop('item_cnt_day',axis=1).columns)\n",
    "for i in _test:\n",
    "    assert i in _training\n",
    "for i in _training:\n",
    "    assert i in _test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "_uuid": "bc1161e2195660162346e34a4e5e78209f3109ad"
   },
   "outputs": [],
   "source": [
    "assert _training == _test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "_uuid": "d98523fce97aa37a4e0ce925b0d6e21b6da7766c"
   },
   "outputs": [],
   "source": [
    "test = test.drop(['ID', 'item_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "_uuid": "59f090d44635310bc0ea6e1e7b0965f3d266a12e"
   },
   "outputs": [],
   "source": [
    "for feat in test.columns:\n",
    "    if 'item_cnt' in feat:\n",
    "        test[feat]=test[feat].fillna(0)\n",
    "    elif 'item_price' in feat:\n",
    "        test[feat]=test[feat].fillna(test[feat].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "38257e1daa45c8dbb473c7b906863ecf94209886"
   },
   "source": [
    "Código para verificar que la base de prueba se generó sin errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "_uuid": "66e4ec6ca217158e7271390cf9e96fed34077462"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_cnt_day_lag_1</th>\n",
       "      <th>item_cnt_day_lag_2</th>\n",
       "      <th>item_cnt_day_lag_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>5320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>5233</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shop_id  item_id  item_cnt_day_lag_1  item_cnt_day_lag_2  \\\n",
       "0        5     5037                 0.0                 1.0   \n",
       "1        5     5320                 0.0                 0.0   \n",
       "2        5     5233                 1.0                 3.0   \n",
       "3        5     5232                 0.0                 0.0   \n",
       "4        5     5268                 0.0                 0.0   \n",
       "\n",
       "   item_cnt_day_lag_3  \n",
       "0                 3.0  \n",
       "1                 0.0  \n",
       "2                 1.0  \n",
       "3                 1.0  \n",
       "4                 0.0  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[['shop_id','item_id']+['item_cnt_day_lag_'+str(x) for x in [1,2,3]]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "_uuid": "8acb9e78a2bea0f0bffae4e80bc5d09c09ff1b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10849431    0.0\n",
      "Name: item_cnt_day, dtype: float64\n",
      "10605772    1.0\n",
      "Name: item_cnt_day, dtype: float64\n",
      "10396094    3.0\n",
      "Name: item_cnt_day, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(training[training['shop_id'] == 5][training['item_id'] == 5037][training['date_block_num'] == 33]['item_cnt_day'])\n",
    "print(training[training['shop_id'] == 5][training['item_id'] == 5037][training['date_block_num'] == 32]['item_cnt_day'])\n",
    "print(training[training['shop_id'] == 5][training['item_id'] == 5037][training['date_block_num'] == 31]['item_cnt_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"Data_Modelos/Prueba_Modelos.csv\",index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de modelos\n",
    "\n",
    "En esta sección se entrenaron tres tipos de modelos: una regresión Ridge, una regresión Lasso y un XGBOOST. El mejor modelo en términos de la raíz cuadrada del error medio cuadrado fue el XGBOOST y el peor fue la regresión lineal. Siendo que el primero es más flexible y el segundo el más rígido. El desempeño de  Lasso y Ridge estuvo entre estos dos modelos, lo que indica que la regularización de los parámetros es adecuada.\n",
    "\n",
    "En la siguiente sección de Evaluación de Modelos se utilizará el modelo XGBOOST para ajustarse sobre todo el conjunto de entrenamiento. Posteriormente, se ajustará dicho modelo en el conjunto de prueba para predecir las ventas por artículo, tienda y mes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
